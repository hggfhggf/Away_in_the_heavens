{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "734d3481",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T08:53:32.073229Z",
     "iopub.status.busy": "2025-10-26T08:53:32.072816Z",
     "iopub.status.idle": "2025-10-26T08:54:36.544505Z",
     "shell.execute_reply": "2025-10-26T08:54:36.543133Z"
    },
    "papermill": {
     "duration": 64.477927,
     "end_time": "2025-10-26T08:54:36.546531",
     "exception": false,
     "start_time": "2025-10-26T08:53:32.068604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000645 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3981\n",
      "[LightGBM] [Info] Number of data points in the train set: 3840, number of used features: 21\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Info] Start training from score 6.266775\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[Fold 1] CB RMSE=0.65719  LGBM RMSE=0.66621\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000536 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3980\n",
      "[LightGBM] [Info] Number of data points in the train set: 3840, number of used features: 21\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Info] Start training from score 6.266255\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[Fold 2] CB RMSE=0.63572  LGBM RMSE=0.65022\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000537 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3980\n",
      "[LightGBM] [Info] Number of data points in the train set: 3840, number of used features: 21\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Info] Start training from score 6.267827\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[Fold 3] CB RMSE=0.61765  LGBM RMSE=0.63656\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3977\n",
      "[LightGBM] [Info] Number of data points in the train set: 3840, number of used features: 21\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Info] Start training from score 6.264975\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[Fold 4] CB RMSE=0.64970  LGBM RMSE=0.66317\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000875 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3983\n",
      "[LightGBM] [Info] Number of data points in the train set: 3840, number of used features: 21\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Info] Start training from score 6.266887\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[Fold 5] CB RMSE=0.61877  LGBM RMSE=0.63967\n",
      "\n",
      "=== Cross-Validation summary (OOF) ===\n",
      "CatBoost OOF RMSE: 0.636007\n",
      "LightGBM OOF RMSE: 0.651277\n",
      "\n",
      "Meta weights (LR): [0.88511815 0.13905084]  Intercept: -0.15922601904015288\n",
      "Stacked OOF RMSE: 0.635447\n",
      "\n",
      "Top features:\n",
      "               feature  importance\n",
      "     fertilizer_amount         958\n",
      "        total_rainfall         905\n",
      "       pesticide_usage         875\n",
      "               soil_ph         609\n",
      "         soil_moisture         607\n",
      "       avg_temperature         537\n",
      "      nitrogen_content         489\n",
      "     potassium_content         453\n",
      "        sunlight_hours         441\n",
      "    phosphorus_content         439\n",
      "                    id         393\n",
      "harvest_date_dayofyear         244\n",
      "     harvest_date_week         134\n",
      "  irrigation_frequency         131\n",
      "                season          86\n",
      "          harvest_date          60\n",
      "    harvest_date_month          56\n",
      "                region          41\n",
      "             crop_type          25\n",
      "              field_id          18\n",
      "  harvest_date_quarter           1\n",
      "     harvest_date_year           0\n",
      "\n",
      "Saved: submission.csv\n",
      "   id  yield_tpha\n",
      "0   0    4.494139\n",
      "1   1    7.472037\n",
      "2   2    7.744051\n",
      "3   3    6.180732\n",
      "4   4    5.896001\n",
      "5   5    6.896014\n",
      "6   6    4.012726\n",
      "7   7    6.936055\n",
      "8   8    7.052881\n",
      "9   9    6.333399\n"
     ]
    }
   ],
   "source": [
    "# ====================== Advanced RMSE pipeline for yield_tpha ======================\n",
    "# - Robust validation: StratifiedKFold by target bins (RMSE per fold + OOF)\n",
    "# - Models: CatBoostRegressor + LightGBM (category-aware), stacking via LinearRegression\n",
    "# - Feature eng: auto date parsing -> year/month/dayofyear/week/quarter, missingness flags\n",
    "# - No hardcoded save path: writes \"submission.csv\" in CWD\n",
    "# - Test path fixed as requested: /kaggle/input/crop-yield-prediction-challenge/crop_yield_test.csv\n",
    "# ================================================================================\n",
    "import os, sys, gc, warnings, numpy as np, pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# --------------------------- Utils ---------------------------\n",
    "def rmse(a, b):\n",
    "    return float(np.sqrt(np.mean((np.asarray(a) - np.asarray(b))**2)))\n",
    "\n",
    "def make_stratified_kfold_bins(y, n_bins=10):\n",
    "    # quantile bins for regression stratification\n",
    "    q = np.linspace(0, 1, n_bins + 1)\n",
    "    # handle duplicates: np.unique on quantiles\n",
    "    bins = np.unique(np.quantile(y, q))\n",
    "    # digitize (rightmost bin inclusive)\n",
    "    return np.digitize(y, bins[1:-1], right=True)\n",
    "\n",
    "def try_read(path_list):\n",
    "    for p in path_list:\n",
    "        if os.path.exists(p):\n",
    "            return pd.read_csv(p)\n",
    "    raise FileNotFoundError(f\"Training file not found. Tried:\\n\" + \"\\n\".join(path_list))\n",
    "\n",
    "def parse_dates_inplace(df, prefix_keep=True):\n",
    "    \"\"\"Parse likely date columns in df and add date-derived features.\"\"\"\n",
    "    candidates = [c for c in df.columns if any(k in c.lower() for k in [\"date\", \"time\", \"dt\", \"ts\"])]\n",
    "    # Also try to parse object columns heuristically\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == \"object\" and c not in candidates:\n",
    "            sample = df[c].dropna().astype(str).head(100).tolist()\n",
    "            if any(\"-\" in s or \"/\" in s or \":\" in s for s in sample):\n",
    "                candidates.append(c)\n",
    "    candidates = list(dict.fromkeys(candidates))  # uniq & keep order\n",
    "\n",
    "    for col in candidates:\n",
    "        try:\n",
    "            dt = pd.to_datetime(df[col], errors=\"coerce\", utc=False, infer_datetime_format=True)\n",
    "            ok_ratio = (~dt.isna()).mean()\n",
    "            if ok_ratio < 0.5:  # too many NaT -> skip\n",
    "                continue\n",
    "            base = col if prefix_keep else \"dt_\" + col\n",
    "            df[f\"{base}_year\"]       = dt.dt.year.astype(\"Int64\")\n",
    "            df[f\"{base}_month\"]      = dt.dt.month.astype(\"Int64\")\n",
    "            df[f\"{base}_dayofyear\"]  = dt.dt.dayofyear.astype(\"Int64\")\n",
    "            df[f\"{base}_week\"]       = dt.dt.isocalendar().week.astype(\"Int64\")\n",
    "            df[f\"{base}_quarter\"]    = dt.dt.quarter.astype(\"Int64\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def add_missingness_flags(df, num_cols):\n",
    "    for c in num_cols:\n",
    "        if df[c].isna().any():\n",
    "            df[f\"{c}__isna\"] = df[c].isna().astype(np.int8)\n",
    "\n",
    "def unify_categories(train, test, cat_cols):\n",
    "    \"\"\"Make sure train/test have identical categorical categories (LightGBM friendly).\"\"\"\n",
    "    for c in cat_cols:\n",
    "        tr = train[c].astype(\"category\")\n",
    "        te = test[c].astype(\"category\")\n",
    "        cats = pd.Index(sorted(set(tr.astype(str)).union(set(te.astype(str)))))\n",
    "        train[c] = pd.Categorical(tr.astype(str), categories=cats)\n",
    "        test[c]  = pd.Categorical(te.astype(str), categories=cats)\n",
    "\n",
    "# --------------------------- Paths ---------------------------\n",
    "TRAIN_CANDIDATES = [\n",
    "    \"/kaggle/input/crop-yield-prediction-challenge/crop_yield_train.csv\",\n",
    "    \"/mnt/data/crop_yield_train.csv\",  # fallback (если вы запускали локально)\n",
    "]\n",
    "TEST_PATH = \"/kaggle/input/crop-yield-prediction-challenge/crop_yield_test.csv\"\n",
    "SAMPLE_SUB_CANDIDATES = [\n",
    "    \"/kaggle/input/crop-yield-prediction-challenge/sample_submission.csv\",\n",
    "    \"/mnt/data/sample_submission.csv\",\n",
    "]\n",
    "\n",
    "# --------------------------- Load ---------------------------\n",
    "train = try_read(TRAIN_CANDIDATES)\n",
    "test  = pd.read_csv(TEST_PATH)\n",
    "try:\n",
    "    sample_sub = try_read(SAMPLE_SUB_CANDIDATES)\n",
    "except Exception:\n",
    "    # Если файла sample_submission нет, создадим болванку из первого столбца теста\n",
    "    first_col = test.columns[0]\n",
    "    sample_sub = pd.DataFrame({first_col: test[first_col].values, \"yield_tpha\": 0.0})\n",
    "\n",
    "TARGET = \"yield_tpha\"\n",
    "assert TARGET in train.columns, f\"`{TARGET}` нет в колонках train: {train.columns.tolist()}\"\n",
    "\n",
    "id_col = sample_sub.columns[0]\n",
    "if id_col not in test.columns:\n",
    "    id_col = \"id\" if \"id\" in test.columns else test.columns[0]\n",
    "\n",
    "# выбросим строки без таргета\n",
    "train = train[~train[TARGET].isna()].copy()\n",
    "\n",
    "# --------------------------- Basic FE ---------------------------\n",
    "# Парсим даты (safe)\n",
    "parse_dates_inplace(train, prefix_keep=True)\n",
    "parse_dates_inplace(test,  prefix_keep=True)\n",
    "\n",
    "# синхронизируем столбцы между train/test (кроме таргета)\n",
    "feat_cols = [c for c in train.columns if c != TARGET]\n",
    "missing_in_test  = [c for c in feat_cols if c not in test.columns]\n",
    "missing_in_train = [c for c in test.columns if c not in feat_cols and c != TARGET]\n",
    "\n",
    "for c in missing_in_test:\n",
    "    test[c] = np.nan\n",
    "for c in missing_in_train:\n",
    "    # добавляем пустые в train, если вдруг есть фичи, которых нет в train, но есть в test\n",
    "    if c != TARGET:\n",
    "        train[c] = np.nan\n",
    "\n",
    "# итоговый список фич одинаков\n",
    "feat_cols = [c for c in train.columns if c != TARGET]\n",
    "train = train[feat_cols + [TARGET]]\n",
    "test  = test[feat_cols]\n",
    "\n",
    "# типы\n",
    "num_cols = train[feat_cols].select_dtypes(include=[np.number, \"Int64\", \"Float64\"]).columns.tolist()\n",
    "cat_cols = [c for c in feat_cols if c not in num_cols]\n",
    "\n",
    "# индикаторы пропусков по числовым\n",
    "add_missingness_flags(train, num_cols)\n",
    "add_missingness_flags(test, num_cols)\n",
    "\n",
    "# обновим списки фич\n",
    "feat_cols = [c for c in train.columns if c != TARGET]\n",
    "num_cols = train[feat_cols].select_dtypes(include=[np.number, \"Int64\", \"Float64\"]).columns.tolist()\n",
    "cat_cols = [c for c in feat_cols if c not in num_cols]\n",
    "\n",
    "# приведение категорий\n",
    "for c in cat_cols:\n",
    "    train[c] = train[c].astype(\"category\")\n",
    "    test[c]  = test[c].astype(\"category\")\n",
    "unify_categories(train, test, cat_cols)\n",
    "\n",
    "# --------------------------- CV setup ---------------------------\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "y = train[TARGET].values\n",
    "bins = make_stratified_kfold_bins(y, n_bins=10)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "X = train[feat_cols].reset_index(drop=True)\n",
    "y = train[TARGET].reset_index(drop=True).astype(float)\n",
    "X_test = test[feat_cols].reset_index(drop=True)\n",
    "\n",
    "# --------------------------- Models ---------------------------\n",
    "use_catboost = True\n",
    "try:\n",
    "    from catboost import CatBoostRegressor, Pool\n",
    "except Exception:\n",
    "    use_catboost = False\n",
    "\n",
    "use_lightgbm = True\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    from lightgbm import LGBMRegressor\n",
    "except Exception:\n",
    "    use_lightgbm = False\n",
    "\n",
    "if not (use_catboost or use_lightgbm):\n",
    "    raise ImportError(\"Need at least one of CatBoost or LightGBM installed in the Kaggle environment.\")\n",
    "\n",
    "# параметры (чуть консервативные, но сильные; правьте при желании)\n",
    "cb_params = dict(\n",
    "    loss_function=\"RMSE\",\n",
    "    eval_metric=\"RMSE\",\n",
    "    iterations=5000,\n",
    "    depth=8,\n",
    "    learning_rate=0.03,\n",
    "    l2_leaf_reg=3.0,\n",
    "    random_seed=SEED,\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=200,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "lgb_params = dict(\n",
    "    objective=\"regression\",\n",
    "    metric=\"rmse\",\n",
    "    n_estimators=10000,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=-1,\n",
    "    num_leaves=63,\n",
    "    min_data_in_leaf=40,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.1,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# --------------------------- OOF training ---------------------------\n",
    "oof_cb = np.zeros(len(X))\n",
    "oof_lgb = np.zeros(len(X))\n",
    "pred_cb = np.zeros(len(X_test))\n",
    "pred_lgb = np.zeros(len(X_test))\n",
    "\n",
    "fold_rmse_cb, fold_rmse_lgb = [], []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(cv.split(X, bins), 1):\n",
    "    X_tr, X_va = X.iloc[tr_idx].copy(), X.iloc[va_idx].copy()\n",
    "    y_tr, y_va = y.iloc[tr_idx].values, y.iloc[va_idx].values\n",
    "\n",
    "    # Ensure the same categories in fold slices\n",
    "    for c in cat_cols:\n",
    "        X_tr[c] = X_tr[c].astype(\"category\")\n",
    "        X_va[c] = X_va[c].astype(\"category\")\n",
    "        X_tr[c] = X_tr[c].cat.set_categories(X[c].cat.categories)\n",
    "        X_va[c] = X_va[c].cat.set_categories(X[c].cat.categories)\n",
    "\n",
    "    # --------- CatBoost ---------\n",
    "    if use_catboost:\n",
    "        tr_pool = Pool(X_tr, y_tr, cat_features=[X_tr.columns.get_loc(c) for c in cat_cols])\n",
    "        va_pool = Pool(X_va, y_va, cat_features=[X_va.columns.get_loc(c) for c in cat_cols])\n",
    "        model_cb = CatBoostRegressor(**cb_params)\n",
    "        model_cb.fit(tr_pool, eval_set=va_pool, verbose=False)\n",
    "        pred_va = model_cb.predict(X_va)\n",
    "        pred_te = model_cb.predict(X_test)\n",
    "        oof_cb[va_idx] = pred_va\n",
    "        pred_cb += pred_te / cv.n_splits\n",
    "        fold_rmse_cb.append(rmse(y_va, pred_va))\n",
    "\n",
    "    # --------- LightGBM ---------\n",
    "    if use_lightgbm:\n",
    "        # LightGBM понимает category dtype напрямую\n",
    "        model_lgb = LGBMRegressor(**lgb_params)\n",
    "        model_lgb.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, y_va)],\n",
    "            eval_metric=\"rmse\",\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=200, verbose=False),\n",
    "            ],\n",
    "        )\n",
    "        pred_va = model_lgb.predict(X_va, num_iteration=model_lgb.best_iteration_)\n",
    "        pred_te = model_lgb.predict(X_test, num_iteration=model_lgb.best_iteration_)\n",
    "        oof_lgb[va_idx] = pred_va\n",
    "        pred_lgb += pred_te / cv.n_splits\n",
    "        fold_rmse_lgb.append(rmse(y_va, pred_va))\n",
    "\n",
    "    print(f\"[Fold {fold}] \"\n",
    "          + (f\"CB RMSE={fold_rmse_cb[-1]:.5f}  \" if use_catboost else \"\")\n",
    "          + (f\"LGBM RMSE={fold_rmse_lgb[-1]:.5f}\" if use_lightgbm else \"\"))\n",
    "\n",
    "# --------------------------- CV summary ---------------------------\n",
    "metrics_log = []\n",
    "if use_catboost:\n",
    "    cv_cb = rmse(y, oof_cb)\n",
    "    metrics_log.append((\"CatBoost OOF RMSE\", cv_cb))\n",
    "if use_lightgbm:\n",
    "    cv_lgb = rmse(y, oof_lgb)\n",
    "    metrics_log.append((\"LightGBM OOF RMSE\", cv_lgb))\n",
    "\n",
    "print(\"\\n=== Cross-Validation summary (OOF) ===\")\n",
    "for name, val in metrics_log:\n",
    "    print(f\"{name}: {val:.6f}\")\n",
    "\n",
    "# --------------------------- Stacking (level-2) ---------------------------\n",
    "from sklearn.linear_model import LinearRegression\n",
    "stack_feats = []\n",
    "stack_test = []\n",
    "if use_catboost: stack_feats.append(oof_cb.reshape(-1,1)); stack_test.append(pred_cb.reshape(-1,1))\n",
    "if use_lightgbm: stack_feats.append(oof_lgb.reshape(-1,1)); stack_test.append(pred_lgb.reshape(-1,1))\n",
    "\n",
    "Z_tr = np.hstack(stack_feats)\n",
    "Z_te = np.hstack(stack_test)\n",
    "\n",
    "meta = LinearRegression(fit_intercept=True)\n",
    "meta.fit(Z_tr, y.values)\n",
    "oof_blend = meta.predict(Z_tr)\n",
    "pred_blend = meta.predict(Z_te)\n",
    "\n",
    "print(\"\\nMeta weights (LR):\", meta.coef_, \" Intercept:\", meta.intercept_)\n",
    "print(f\"Stacked OOF RMSE: {rmse(y, oof_blend):.6f}\")\n",
    "\n",
    "# --------------------------- Feature importance (top-30) ---------------------------\n",
    "def top_importances(model, cols, k=30):\n",
    "    imp = None\n",
    "    try:\n",
    "        if hasattr(model, \"feature_importances_\"):\n",
    "            imp = model.feature_importances_\n",
    "        elif hasattr(model, \"get_feature_importance\"):\n",
    "            imp = model.get_feature_importance()\n",
    "    except Exception:\n",
    "        pass\n",
    "    if imp is not None:\n",
    "        fi = pd.DataFrame({\"feature\": cols, \"importance\": imp})\n",
    "        fi = fi.sort_values(\"importance\", ascending=False).head(k)\n",
    "        print(\"\\nTop features:\")\n",
    "        print(fi.to_string(index=False))\n",
    "    else:\n",
    "        print(\"\\n(Feature importance not available for this model)\")\n",
    "\n",
    "# покажем важности последней обученной модели LGBM/CB (если они есть)\n",
    "if use_lightgbm:\n",
    "    top_importances(model_lgb, X.columns.tolist(), k=30)\n",
    "elif use_catboost:\n",
    "    top_importances(model_cb, X.columns.tolist(), k=30)\n",
    "\n",
    "# --------------------------- Submission ---------------------------\n",
    "# Берём стек (обычно лучше OOF-мета), но при желании можно усреднить с лучшей моделью\n",
    "final_pred = pred_blend\n",
    "\n",
    "sub = sample_sub.copy()\n",
    "if id_col in sub.columns and id_col in test.columns:\n",
    "    sub = sub[[id_col]].merge(\n",
    "        pd.DataFrame({id_col: test[id_col].values, \"yield_tpha\": final_pred}),\n",
    "        on=id_col, how=\"left\"\n",
    "    )\n",
    "else:\n",
    "    # fallback: если sample_sub странный\n",
    "    key = sub.columns[0]\n",
    "    sub[key] = test[key].values\n",
    "    if \"yield_tpha\" not in sub.columns:\n",
    "        sub[\"yield_tpha\"] = final_pred\n",
    "    else:\n",
    "        sub[\"yield_tpha\"] = final_pred\n",
    "\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nSaved: submission.csv\")\n",
    "print(sub.head(10))\n",
    "# ================================================================================\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14167899,
     "sourceId": 118284,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 70.911442,
   "end_time": "2025-10-26T08:54:37.672511",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-26T08:53:26.761069",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
